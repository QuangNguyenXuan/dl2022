{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efe5b00-cda0-4c3f-9cbc-6fd590ebb4a6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22382432-34a8-474b-9519-af1168597ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7aaa78c-18e0-4e05-bfed-e22b1b7194bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer function\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Create a function to yield list of tokens\n",
    "def yield_tokens(examples):\n",
    "    for text in examples:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Tokenize and numericalize your samples\n",
    "def vectorize_en(text, vocab, sequence_length):\n",
    "    tokens = tokenizer(text)\n",
    "    tokens = [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
    "    token_ids = tokens[:sequence_length] + [vocab[\"<pad>\"]] * (sequence_length - len(tokens))\n",
    "    return token_ids\n",
    "\n",
    "def vectorize_vn(text, vocab, sequence_length):\n",
    "    tokens = tokenizer(text)\n",
    "    tokens = [vocab[\"<sos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
    "    token_ids = tokens[:sequence_length] + [vocab[\"<pad>\"]] * (sequence_length - len(tokens))\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b52f74-b11f-4edc-9dc1-af4d396cce68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a84e42f-922c-4060-a00b-85dd862b1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = [\n",
    "    \"good morning\",\n",
    "    \"ai books\"    \n",
    "]\n",
    "data_size_en = len(corpus_en)\n",
    "\n",
    "# max vocabulary size and sequence length\n",
    "vocab_size_en = 7\n",
    "sequence_length_en = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "838ccb88-1930-47ba-8536-c97bb58b9fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'books': 4,\n",
       " '<unk>': 0,\n",
       " '<eos>': 2,\n",
       " '<pad>': 1,\n",
       " 'morning': 6,\n",
       " 'ai': 3,\n",
       " 'good': 5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "vocab_en = build_vocab_from_iterator(yield_tokens(corpus_en),\n",
    "                                     max_tokens=vocab_size_en,\n",
    "                                     specials=[\"<unk>\", \"<pad>\", \"<eos>\"])\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "vocab_en.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53c1877c-fd26-4a6f-9bd3-ff50a1e3ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 2],\n",
      "        [3, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the samples\n",
    "corpus_ids_en = []\n",
    "for sentence in corpus_en:\n",
    "    corpus_ids_en.append(vectorize_en(sentence, vocab_en, sequence_length_en))\n",
    "\n",
    "# print\n",
    "en_data = torch.tensor(corpus_ids_en, dtype=torch.long)\n",
    "print(en_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb825e84-f192-4944-a00a-0f4c19a37fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd8bc9-09c2-4d48-9df7-58509f5a5844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221da0e-a1c6-4944-b4a8-05b7f66b3cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "164459c8-b226-447a-8a69-4a12e05e7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vn = [\n",
    "    \"chào buổi sáng\",\n",
    "    \"sách ai\"    \n",
    "]\n",
    "data_size_vn = len(corpus_vn)\n",
    "\n",
    "# max vocabulary size and sequence length\n",
    "vocab_size_vn = 9\n",
    "sequence_length_vn = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2726650a-52ef-4150-9b09-0071d0ce2a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " 'chào': 6,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " '<sos>': 2,\n",
       " 'ai': 4,\n",
       " 'buổi': 5,\n",
       " 'sách': 7,\n",
       " 'sáng': 8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "vocab_vn = build_vocab_from_iterator(yield_tokens(corpus_vn),\n",
    "                                  max_tokens=vocab_size_vn,\n",
    "                                  specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab_vn.set_default_index(vocab_vn[\"<unk>\"])\n",
    "vocab_vn.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548484cb-e415-4e4f-a7f5-b0fd0ed48e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6, 5, 8, 3], [2, 7, 4, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the samples\n",
    "corpus_ids_vn = []\n",
    "for sentence in corpus_vn:\n",
    "    corpus_ids_vn.append(vectorize_vn(sentence, vocab_vn, sequence_length_vn+1))\n",
    "\n",
    "# print\n",
    "print(corpus_ids_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c610df2e-db78-4110-b8e5-2df61ffbff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 6, 5, 8],\n",
      "        [2, 7, 4, 3]])\n",
      "tensor([[6, 5, 8, 3],\n",
      "        [7, 4, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "input_vn_data = []\n",
    "label_vn_data = []\n",
    "\n",
    "for vector in corpus_ids_vn:\n",
    "    input_vn_data.append(vector[:-1])\n",
    "    label_vn_data.append(vector[1:])  \n",
    "\n",
    "# convert to tensors\n",
    "input_vn_data = torch.tensor(input_vn_data, dtype=torch.long)\n",
    "label_vn_data = torch.tensor(label_vn_data, dtype=torch.long)\n",
    "\n",
    "# print\n",
    "print(input_vn_data)\n",
    "print(label_vn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b2726-3c28-4880-9064-9a35fbe11d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b547154b-2223-4e4f-a875-119ec33f9812",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f159d4-4a24-4cd2-bd45-853802e1ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size_en, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size_en, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    # src: [batch_size, seq_length]\n",
    "    def forward(self, src): \n",
    "        embedded = self.embedding(src)  # [batch_size, seq_length, embedding_dim]        \n",
    "        _, hidden = self.rnn(embedded)  # [1, batch_size, hidden_dim]        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c940aa66-82d6-448e-a349-2ed223cd065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim, hidden_dim = 6, 6\n",
    "encoder = Encoder(vocab_size_en, embedding_dim, hidden_dim)\n",
    "\n",
    "hidden_sample = encoder(en_data)\n",
    "print(hidden_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e38434-1d7a-4c22-b278-ec03481377ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39687bf5-3741-4910-8444-c89358bc81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size_vn, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size_vn, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size_vn)\n",
    "\n",
    "    # input: [batch_size, seq_length]\n",
    "    # hidden: [1, batch_size, hidden_dim]\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)          # [batch_size, seq_length, embedding_dim]        \n",
    "        output, _ = self.rnn(embedded, hidden)    # [batch_size, seq_length, hidden_dim]\n",
    "        prediction = self.fc_out(output)          # [batch_size, vocab_size_vn]\n",
    "        \n",
    "        return prediction.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ae940e0-e7bf-43c3-af4c-45ea79ffac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size_vn, embedding_dim, hidden_dim)\n",
    "outputs = decoder(input_vn_data, hidden_sample)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328c8e3-1ed9-4df0-9bc5-f65a0871c47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eb95237-d3b6-4f42-9f3e-25aadd967a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, sequence_en, sequence_vn):        \n",
    "        hidden = self.encoder(sequence_en)\n",
    "        outputs = self.decoder(sequence_vn, hidden)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cb8b50-51a1-43b1-9d2d-569c269e3ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq_Model(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(7, 6)\n",
      "    (rnn): RNN(6, 6, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(9, 6)\n",
      "    (rnn): RNN(6, 6, batch_first=True)\n",
      "    (fc_out): Linear(in_features=6, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq_Model(encoder, decoder)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1367b830-30f1-43e1-bef1-e896633345ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(en_data, input_vn_data)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9f125-0d43-4248-a3b7-3405fd86d6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec415bd4-83c5-44f1-9be5-40e7b46a82e4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32ed2deb-b3e3-4a70-8b85-c575001fc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbee29f3-5988-4f5f-a9f1-fb13f137a91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.249560594558716\n",
      "1.9828377962112427\n",
      "1.7630938291549683\n",
      "1.5635037422180176\n",
      "1.3691974878311157\n",
      "1.190489649772644\n",
      "1.036076307296753\n",
      "0.9050320386886597\n",
      "0.791242241859436\n",
      "0.6925269961357117\n",
      "0.6101727485656738\n",
      "0.5432384014129639\n",
      "0.48622769117355347\n",
      "0.43204283714294434\n",
      "0.3798092007637024\n",
      "0.33621659874916077\n",
      "0.3014173209667206\n",
      "0.27165430784225464\n",
      "0.24489396810531616\n",
      "0.2201252430677414\n",
      "0.19674460589885712\n",
      "0.17500600218772888\n",
      "0.15543237328529358\n",
      "0.13775195181369781\n",
      "0.1215820461511612\n",
      "0.10711026191711426\n",
      "0.09449587017297745\n",
      "0.08354666829109192\n",
      "0.07402341067790985\n",
      "0.06580005586147308\n",
      "0.058771610260009766\n",
      "0.052790336310863495\n",
      "0.047688186168670654\n",
      "0.04331155866384506\n",
      "0.0395352765917778\n"
     ]
    }
   ],
   "source": [
    "for _ in range(35):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(en_data, input_vn_data)\n",
    "    loss = criterion(outputs, label_vn_data)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b562b790-b1f8-4e0c-a1cc-2453ee3f3a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 5, 8, 3],\n",
      "        [7, 4, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(en_data, input_vn_data)\n",
    "print(torch.argmax(outputs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "051f948f-3123-43f0-8e9d-ec8315314d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 5, 8, 3],\n",
       "        [7, 4, 3, 1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f519bd6-a4f9-4037-bea8-bf7118e684f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
