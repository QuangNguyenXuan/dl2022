{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d18b44e-52dd-42ff-9827-6442a7f11550",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c60c4-ccb8-403c-b16a-7266f9a1ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "\n",
    "# Define tokenizer function\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "sample1_tokens = tokenizer(sample1)\n",
    "sample2_tokens = tokenizer(sample2)\n",
    "\n",
    "print(sample1_tokens)\n",
    "print(sample2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125ed1e-9f13-408c-8f79-9bb4bdc58572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4005583-8455-4cc0-a9e5-24fa8564889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem?\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "data = [sample1, sample2, ...]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_size = 8\n",
    "vocab = build_vocab_from_iterator(data)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e520cc8-13df-4b27-9957-49bbe15f40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem?\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "data = [sample1, sample2, ...]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_size = 8\n",
    "vocab = build_vocab_from_iterator(data,\n",
    "                                  max_tokens=vocab_size,\n",
    "                                  specials=[\"<unk>\", \n",
    "                                            \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700ffe54-f581-45aa-98a3-441c1dc85b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "data = [sample1, sample2]\n",
    "\n",
    "# Create a function to yield list of tokens\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(examples):\n",
    "    for text in examples:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_size = 8\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data),\n",
    "                                  max_tokens=vocab_size,\n",
    "                                  specials=[\"<unk>\", \n",
    "                                            \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e337b92e-bf41-4d10-afbe-096418095da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " 'ai': 2,\n",
       " 'a': 3,\n",
       " 'is': 6,\n",
       " 'are': 4,\n",
       " 'learning': 7,\n",
       " 'cs': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3ac4d-3a85-4d56-85ca-e8ee9c969a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f1d59f-c4b5-42d1-bb56-54b51ea50c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'learning', 'ai']\n",
      "[0, 4, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sample1)\n",
    "print(tokens) \n",
    "\n",
    "sample1_tokens = [vocab[token] for token in tokens]\n",
    "print(sample1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495dad80-c3fb-4635-9d67-c7897c601c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'is', 'a', 'cs', 'topic']\n",
      "[2, 6, 3, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sample2)\n",
    "print(tokens) \n",
    "\n",
    "sample2_tokens = [vocab[token] for token in tokens]\n",
    "print(sample2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c175a8-1309-4e38-88a5-b678318514b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c7e2f9-eba6-4456-b3aa-46d78b394b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Sample 1: tensor([0, 4, 7, 2, 1])\n",
      "Vectorized Sample 2: tensor([2, 6, 3, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "# problem?\n",
    "import torch\n",
    "\n",
    "# Tokenize and numericalize your samples\n",
    "def vectorize(text, vocab, sequence_length):\n",
    "    tokens = tokenizer(text)    \n",
    "    tokens = [vocab[token] for token in tokens] \n",
    "    \n",
    "    num_pads = sequence_length - len(tokens)\n",
    "    tokens = tokens + [vocab[\"<pad>\"]] * num_pads\n",
    "    \n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Vectorize the samples\n",
    "sequence_length = 5\n",
    "vectorized_sample1 = vectorize(sample1, \n",
    "                               vocab, \n",
    "                               sequence_length)\n",
    "vectorized_sample2 = vectorize(sample2, \n",
    "                               vocab, \n",
    "                               sequence_length)\n",
    "\n",
    "print(\"Vectorized Sample 1:\", vectorized_sample1)\n",
    "print(\"Vectorized Sample 2:\", vectorized_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72f1c95-d8b3-4684-8278-6931e52b6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 0, 5, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "sample3 = 'AI topic in CS is difficult'\n",
    "vectorized_sample3 = vectorize(sample3, \n",
    "                               vocab, \n",
    "                               sequence_length)\n",
    "print(vectorized_sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f95c0e-bf74-45fa-9650-82ec9f31a977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ae1cfe-b2a0-44d1-99ce-f537a74ab151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Sample 1: tensor([0, 4, 7, 2, 1])\n",
      "Vectorized Sample 2: tensor([2, 6, 3, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vectorize(text, vocab, seq_len):\n",
    "    tokens = tokenizer(text)    \n",
    "    tokens = [vocab[token] for token in tokens] \n",
    "    \n",
    "    num_pads = sequence_length - len(tokens)\n",
    "    tokens = tokens[:sequence_length] \n",
    "             + [vocab[\"<pad>\"]]*num_pads\n",
    "    \n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Vectorize the samples\n",
    "sequence_length = 5\n",
    "vectorized_sample1 = vectorize(sample1, vocab, \n",
    "                               sequence_length)\n",
    "vectorized_sample2 = vectorize(sample2, vocab, \n",
    "                               sequence_length)\n",
    "\n",
    "print(\"Vectorized Sample 1:\", vectorized_sample1)\n",
    "print(\"Vectorized Sample 2:\", vectorized_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897b7f8e-1118-45e5-9b8d-6c31a7fe4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 0, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "sample3 = 'AI topic in CS is difficult'\n",
    "vectorized_sample3 = vectorize(sample3, vocab, \n",
    "                               sequence_length)\n",
    "print(vectorized_sample3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a026fa8-8860-49e1-a621-18596279af5d",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa0ebb-c8e0-4ef3-9ea1-bdca2c51c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 8\n",
    "embed_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.1882,  0.5530,  1.6267,  0.7013],\n",
    "                                [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
    "                                [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
    "                                [-1.3083, -0.0987,  0.7647, -0.3680],\n",
    "                                [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
    "                                [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
    "                                [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
    "                                [ 0.4309, -1.3067, -0.8823,  1.5977]]).float()\n",
    "embedding.weight = nn.Parameter(custom_weights)\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a027791-dc92-4ac7-8fb7-20c68d528a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59e8091c-b48c-479e-afb4-500f25f75835",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab9d2b8-bc27-45ff-9860-27d5eb74d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1882,  0.5530,  1.6267,  0.7013],\n",
      "        [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
      "        [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
      "        [-1.3083, -0.0987,  0.7647, -0.3680],\n",
      "        [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
      "        [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
      "        [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
      "        [ 0.4309, -1.3067, -0.8823,  1.5977]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 8\n",
    "embed_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, \n",
    "                         embed_dim)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.1882,  0.5530,  1.6267,  0.7013],\n",
    "                                [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
    "                                [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
    "                                [-1.3083, -0.0987,  0.7647, -0.3680],\n",
    "                                [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
    "                                [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
    "                                [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
    "                                [ 0.4309, -1.3067, -0.8823,  1.5977]]).float()\n",
    "embedding.weight = nn.Parameter(custom_weights)\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcffa230-ef6c-450d-a781-e9b20f727dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([0, 4, 7, 2, 1], dtype=torch.long)\n",
    "label = torch.tensor([1], dtype=torch.float)\n",
    "\n",
    "x = embedding(data)\n",
    "x = nn.Flatten(0)(x)\n",
    "x = nn.Linear(20, 1)(x)\n",
    "output = nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49429089-131c-414f-94a6-4a2dccd17b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1892,  0.5540,  1.6257,  0.7023],\n",
      "        [ 1.7850, -0.8288, -0.2691,  1.3596],\n",
      "        [ 1.0271, -1.9104,  0.3192,  0.4201],\n",
      "        [-1.3083, -0.0987,  0.7647, -0.3680],\n",
      "        [ 0.2283,  1.3265,  0.1308,  2.0491],\n",
      "        [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
      "        [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
      "        [ 0.4319, -1.3057, -0.8833,  1.5987]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss()(output, label)\n",
    "optimizer = torch.optim.Adam(embedding.parameters())\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e250447-6ebf-4cf2-ae22-19c2c282be1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e6a78-e0df-4e8c-9217-a566f6c31756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb88848-999f-457e-88f7-0991afdf18ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
